{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"traditional_feature_extraction.ipynb","provenance":[],"machine_shape":"hm","collapsed_sections":["m0Ucc8ym3FdZ"],"authorship_tag":"ABX9TyOb+UZ1Zelba69LB3J+X4hg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\n","\n","---\n","\n","#### Note: Throughout the thesis code/notebooks, to reproduce different results and methods, code cells are edited and desired paramteres entered and re-ran. Code is commented out and in at times when we want to use different variables etc, this saves having lots of repeated code clogging up the notebooks. Output from cells is not always maintained.\n","\n","\n","---\n","\n"],"metadata":{"id":"6krNss99cjDR"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n","# Installations & Imports\n","\n","\n","---\n","\n"],"metadata":{"id":"m0Ucc8ym3FdZ"}},{"cell_type":"code","source":["!pip install tensorflow-io\n","!pip install pydub\n","!pip install wget"],"metadata":{"id":"HlyVNo9rMOSA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649094174955,"user_tz":-60,"elapsed":12749,"user":{"displayName":"chris o'keeffe","userId":"06758292410552909315"}},"outputId":"9f18f93b-29ce-49bc-d6c5-63236445f604"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorflow-io\n","  Downloading tensorflow_io-0.24.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (23.4 MB)\n","\u001b[K     |████████████████████████████████| 23.4 MB 1.2 MB/s \n","\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem==0.24.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-io) (0.24.0)\n","Installing collected packages: tensorflow-io\n","Successfully installed tensorflow-io-0.24.0\n","Collecting pydub\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Installing collected packages: pydub\n","Successfully installed pydub-0.25.1\n","Collecting wget\n","  Downloading wget-3.2.zip (10 kB)\n","Building wheels for collected packages: wget\n","  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=d9ebeae6a887935b6a90b70e968daafa873e4317dc392526b97b7b53034d7c74\n","  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n","Successfully built wget\n","Installing collected packages: wget\n","Successfully installed wget-3.2\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","import numpy as np\n","import pandas as pd\n","import librosa\n","import json\n","import os \n","import wget\n","import shutil\n","\n","from pydub import AudioSegment\n","from pydub.playback import play"],"metadata":{"id":"Bjob6HNXJR25"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fTaaanfjIuOV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649094847427,"user_tz":-60,"elapsed":672479,"user":{"displayName":"chris o'keeffe","userId":"06758292410552909315"}},"outputId":"0da2c64c-11c8-4208-baa5-ee9718c32fd4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["drive.mount('/content/gdrive', force_remount=True)"]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","# Functions\n","\n","\n","---\n","\n"],"metadata":{"id":"ZeDdjNq33beg"}},{"cell_type":"code","source":["def create_audio_sample(file, snippet_size=120):\n","  \"\"\"\n","    Input: full podcast episode (mp3)\n","    Output: Extract/stitch desired snippet and export as wav file\n","  \"\"\"\n","\n","  audio = AudioSegment.from_mp3(file)\n","  audio = audio.set_channels(1)\n","\n","  # pydub does things in milliseconds\n","  # Take leading snippet size and a snippet from during the podcast\n","  start = audio[:(snippet_size*1000)]\n","  midpoint = len(audio)//2\n","  middle = audio[midpoint : midpoint + (snippet_size*1000)]\n","\n","  # Export audio sample for preprocessing\n","  sample = start + middle\n","  sample.export('new_file.wav', format=\"wav\")\n","\n","\n","def extract_features(file):\n","  \"\"\"\n","    Input: Wav audio snippet\n","    Output: audio features via librosa\n","  \"\"\"\n","\n","  signal, sr = librosa.load(file)\n","\n","  # Baseline audio features - MFCC, zero crossing rate, spectral ecntroid\n","  mfcc = librosa.feature.mfcc(signal, sr)\n","  zcr = librosa.feature.zero_crossing_rate(signal)\n","  spec_centroid = librosa.feature.spectral_centroid(signal, sr)\n","\n","  return mfcc, zcr, spec_centroid\n","\n","\n","def create_episode_map(file):\n","  \"\"\" \n","    Create a map from IDs -> urls, titles\n","    The popularity train/test sets don't have the URLs.\n","    We use the map to get urls for a give episode from the full set in\n","    podcast_episodes_sb dataset.\n","\n","    Returns: a map of IDs that exist in both datasets\n","  \"\"\"\n","\n","  data = [json.loads(line) for line in open(file, 'r')]\n","\n","  map = {}\n","  for x in data:\n","    map[x['id']] = (x['url'], x['title'])\n","\n","  return map\n","\n"],"metadata":{"id":"xGqrvtXLJVkV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","# Feature Extraction: \n","1. Iteratively process podcast episodes\n","2. Extract baseline traditional features\n","3. Output dataset\n","\n","First time around, this will be time consuming since we have to download files and extract samples for the first time. Files are saved to google drive and the features are dumped into json files for future use.\n","\n","### This is executed in batches by using slices in the for loop, due to colab time limits and constraints etc.\n","---\n","\n"],"metadata":{"id":"qZzceikb6odO"}},{"cell_type":"code","source":["\n","# Map to store ID -> metadata relationships\n","data_map = create_episode_map('gdrive/MyDrive/thesis/podcast_data/podcast_episodes_sb.json')\n","\n","# Labelled train/test subset of podcat episodes from Yang et al\n","# We will be making our own train/test sets once all featurex are extracted\n","popularity_train = [json.loads(line) for line in open('gdrive/MyDrive/thesis/podcast_data/popularity_train.json', 'r')]\n","popularity_test = [json.loads(line) for line in open('gdrive/MyDrive/thesis/podcast_data/popularity_test.json', 'r')]\n","\n","# A dict to curate the output dataset with extracted features\n","dataset = {\n","    \"id\" : [],\n","    \"title\": [],\n","    \"mfcc\": [],\n","    \"zcr\": [],\n","    \"spec_centroid\": [],\n","    \"label\": []\n","}\n","\n","# Debug count\n","x = 0\n","\n","for i in popularity_test[20:21]:\n","  id = i['id']\n","\n","  # if the episode exists in the sampled subset of annotated samples\n","  url = data_map[id][0]\n","  try:\n","    print(url)\n","    file = wget.download(url)\n","    lab = i['label']\n","\n","    # Convert file to wav and create a sample for extracting features\n","    # Literature takes leading minutes\n","    create_audio_sample(file, snippet_size = 150)\n","\n","    # Extract features from created audio sample\n","    mfcc, zcr, spec_centroid = extract_features('new_file.wav')\n","\n","    print(mfcc.shape)\n","\n","    # # Add to datset\n","    dataset['id'].append(id)\n","    dataset['label'].append(lab)\n","    dataset['mfcc'].append(mfcc.T.tolist())\n","    dataset['zcr'].append(zcr.T.tolist())\n","    dataset['spec_centroid'].append(spec_centroid.tolist())\n","    \n","    # # Clean up \n","    os.system(f'rm {file}')\n","    os.system('rm new_file.wav')\n","    # print(x)\n","\n","  except Exception as e:\n","    print(x, e, url)\n","    pass\n","\n","  x += 1\n","\n","  \n","# # Dump dataset into json file\n","# with open('gdrive/MyDrive/thesis/podcast_data/pop_test/popularity_test_features_1400_2400.json', 'w') as fp:\n","#     json.dump(dataset, fp)"],"metadata":{"id":"2ADgWXU96kCo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649095652546,"user_tz":-60,"elapsed":24412,"user":{"displayName":"chris o'keeffe","userId":"06758292410552909315"}},"outputId":"a7a1f26d-6e3e-45ad-be57-0be6a5518756"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["https://media.acast.com/ctrlaltdelete/-87matthaig-twitter-timeandmentalhealth/media.mp3\n","(20, 12920)\n"]}]}]}